<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Mark Liberman – DATASCI 194C/294C</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../styles.css">

    <style>
        /* Speaker layout – conference-style */
        .speaker-section {
            padding-top: 2rem;
            padding-bottom: 2.5rem;
        }

        .speaker-card {
            display: flex;
            gap: 1.5rem;
            align-items: flex-start;
            background: var(--card, #f7f7f7);
            border: 1px solid var(--border, #e5e5e5);
            border-radius: 12px;
            padding: 1.2rem 1.4rem;
            margin-bottom: 2rem;
        }

        .speaker-photo {
            max-width: 220px;
            border-radius: 10px;
            flex-shrink: 0;
        }

        .speaker-main {
            flex: 1 1 auto;
        }

        .speaker-name {
            margin: 0 0 0.4rem;
            font-size: 1.6rem;
        }

        .speaker-affiliation {
            margin: 0 0 0.8rem;
            color: var(--muted, #666);
            font-size: 0.95rem;
        }

        .speaker-meta-label {
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 0.25rem;
            color: var(--muted, #666);
        }

        .speaker-titles {
            padding-left: 1rem;
            margin: 0 0 0.5rem;
            font-size: 0.95rem;
        }

        .speaker-titles li {
            margin-bottom: 2px;
        }

        /* Mobile adjustments */
        @media (max-width: 768px) {
            .container {
                padding-left: 1.4rem;
                padding-right: 1.4rem;
            }

            .speaker-card {
                flex-direction: column;
                align-items: flex-start;
                padding: 1rem 1.2rem;
            }

            .speaker-photo {
                max-width: 180px;
            }
        }
    </style>
</head>

<body>

<header class="topbar">
    <div class="container masthead">
        <div class="title">
            <h1>DATASCI 194C/294C</h1>
        </div>
    </div>
</header>

<div class="container speaker-section">

    <!-- Bio-card -->
    <div class="speaker-card">
        <img src="../liberman.jpg" alt="Mark Liberman" class="speaker-photo">

        <div class="speaker-main">
            <h2 class="speaker-name">Mark Liberman</h2>
            <p class="speaker-affiliation">University of Pennsylvania</p>

            <div>
                <div class="speaker-meta-label">Academic Roles</div>
                <ul class="speaker-titles">
                    <li><strong>Christopher H. Browne Distinguished Professor of Linguistics</strong></li>
                    <li>Professor, Department of Linguistics</li>
                    <li>Professor, Department of Computer and Information Science</li>
                    <li>Director, Linguistic Data Consortium</li>
                    <li>Faculty Director, Ware College House</li>
                </ul>
            </div>
        </div>
    </div>

    <!-- Talk details -->
    <section>
        <h3>Talk Title</h3>
        <p><strong>The Common Task Method: Past, Present, and Future</strong></p>

        <h3>Abstract</h3>

        <p>
            The 
            <a href="https://arxiv.org/abs/2012.01477" target="_blank" rel="noopener noreferrer">“Edisonian approach”</a>
            to invention goes back to the medieval alchemists and beyond – in areas where theory was lacking or incomplete,
            systematic trial and error with quantitative comparison of outcomes was sometimes a practical success.
            After a series of Human Language Technology failures, similar methods were applied in the 1970s and 1980s by
            Fred Jelinek’s group at IBM, by George Doddington’s group at Texas Instruments, and by various speech and language
            groups at Bell Labs, MIT, and elsewhere. These researchers shared the Edisonian belief that HLT is an area where
            engineering can precede science, and where learning from large amounts of data is needed. In order to make
            experimental hill-climbing possible, they devised specific simplified tasks, like recognizing digit strings, and
            they designed simple automatically-calculable metrics like “word error rate” and the
            <a href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="noopener noreferrer">BLEU translation metric</a>.
        </p>

        <p>
            Starting in the mid-1980s, (D)ARPA decided to return to HLT after some earlier failed attempts, and chose to use
            quantitative evaluation methods rather than the “evaluation by demonstration” that they’d used before. Some believed
            in the methodology; others saw it mainly as a way to detect what John Pierce had called 
            <a href="https://iiif.library.cmu.edu/file/Newell_box00030_fld02101_doc0004/Newell_box00030_fld02101_doc0004.pdf" target="_blank" rel="noopener noreferrer">“deceit and glamour”</a>,
            expecting quantitative failure to lead to program termination.
        </p>

        <p>
            This context required an iterative series of task designs that were easy enough to allow measurable progress,
            hard enough to persuade funders of progress toward ultimate goals, and cheap enough to start and finish quickly.
            To avoid being trapped by local biases, multiple research groups should participate, which required public datasets
            and public evaluation metrics. Achieving this required careful joint task design involving funders, researchers,
            and potential consumers.
        </p>

        <p>This resulted in what came to be known as the “Common Task” structure:</p>

        <ul>
            <li>A detailed task definition and evaluation plan published at the start;</li>
            <li>Automatic evaluation software written and maintained by NIST, released at project start;</li>
            <li>Shared training and developmental test data released publicly;</li>
            <li>Evaluation test data withheld for periodic public evaluations.</li>
        </ul>

        <p>Although initially unpopular, the Common Task method succeeded because:</p>

        <ul>
            <li>It secured funding by appearing resistant to “glamour and deceit,” and allowed long-term evaluation years before commercialization.</li>
            <li>It enabled internal experimental hill-climbing via automatic, standardized metrics.</li>
            <li>It created a culture: researchers shared methods and results using common datasets and metrics.</li>
        </ul>

        <p>
            Participation soon became so valuable that many research groups joined without funding. This was the Classical
            Common Task era, which later inspired similar “challenges” across engineering and AI.
        </p>

        <p>
            But some things have been lost in translation, and the modern practice of 
            <a href="https://drive.google.com/file/d/1rJEPy-1FClKv2mcPKitkxHqs9oHI2RzC/view" target="_blank" rel="noopener noreferrer">“Leaderboard Chasing”</a>
            has drawn legitimate criticism. Many concerns raised today were well understood decades ago and motivated practices
            that have since been partially or completely abandoned. This talk suggests ways to restore and improve the Common
            Task method for future applications.
        </p>

        <h3>More Information</h3>
        <p>
            Full bio, publications, and additional resources available at:
            <a href="https://www.ling.upenn.edu/~myl/" target="_blank" rel="noopener noreferrer">
                https://www.ling.upenn.edu/~myl/
            </a>
        </p>
    </section>

</div>

</body>
</html>
